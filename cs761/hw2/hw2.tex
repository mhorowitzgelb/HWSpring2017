% To produce pdf under linux, run
% pdflatex hw2.tex 

\documentclass{article}
\usepackage{amsmath,amssymb}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{url}
\usepackage{color}

\def\bfx{\mathbf x}
\def\R{\mathbb R}
\def\E{\mathbb E}
\def\argmax{\mathrm{argmax}}

\title{CS761 Spring 2015 Homework 2}
\author{Assigned Mar. 13, due Mar. 27 before class}
\date{}
\begin{document}
\maketitle

Instructions: 
\begin{itemize}
\item Homeworks are to be done individually.
\item Typeset your homework in latex using this file as template (e.g. use pdflatex).  Show your derivations.
\item Hand in the compiled pdf (not the latex file) online.  Instructions will be provided.  We do not accept hand-written homeworks.  
\item Homework will no longer be accepted once the lecture starts.
\item Fill in your name and email below.  
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Insert your name and email here:

Name: Max Horowitz-Gelb                     

Email: horowitzgelb@wisc.edu

\newpage % Please keep this page-break
% Do not include any identifying information below this line.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{enumerate}

\item
Let $X_0, X_1, \ldots, X_{M-1}$ denote a random sample of $N$-dimensional random vectors $X_n$, each of which has mean value $m$ and covariance matrix $R$.  Show that the sample mean
$$\hat m_t = \frac{1}{t+1} \sum_{n=0}^t  X_n$$
and the sample covariance 
$$S_t(\hat m_t) = \frac{1}{t+1} \sum_{n=0}^t (X_n - \hat m_t)(X_n - \hat m_t)^\top$$
may be written recursively as
$$\hat m_t = \frac{t}{t+1} \hat m_{t-1} + \frac{1}{t+1} X_t, \quad \hat m_0 = X_0,$$
and
$$S_t(\hat m_t) = Q_t - \hat m_t \hat m_t^\top,$$
where
$$ Q_t  = \frac{t}{t+1} Q_{t-1} + \frac{1}{t+1} X_t X_t^\top.$$

\color{blue}
i.

Assume that for some $t>0$ 
$$
\hat{m}_{t-1} = \frac{t-1}{t} \hat m_{t-2} + \frac{1}{t+1}X_{t-1}
$$

then 

\begin{align*}
\frac{t}{t+1} \hat m_{t-1} + \frac{1}{t+1}X_t\\
= \frac{t}{t+1} \frac{1}{t} \sum_{n=0}^{t-1} X_n + \frac{1}{t+1}X_t\\
= \frac{1}{t+1} \sum_{n=0}^t  X_n \\
=\hat m_t
\end{align*}
Then since clearly
\[
\hat m_1 = \frac{1}{2} \hat m_0 + \frac{1}{2} X_1
\]
Then for all $t>0$

$$\hat m_t = \frac{t}{t+1} \hat m_{t-1} + \frac{1}{t+1} X_t
$$

\color{black}


\item
Suppose we roll a fair 6-sided die 100 times. Let $X$ be the sum of the outcomes.  Bound $P(|X-350| \ge 100)$ using Chebyshev and Hoeffding, respectively.

\color{blue}

$X$ is the sum of $n$ iid outcomes, $Y_1 ... Y_n$.

\[
E[Y_i] = 7/2
\]
\[
Var[Y_i] = 35/12
\]

Therefore
$$E[X] = \sum_{i=1} ^{100} E[Y_i] = 350$$

$$Var[X] = \sum_{i=1} ^{100} Var[Y_i] =  291 + 2/3$$

Therefore by the Chebyshev inequality

\[
Pr(|X-350| \geq 100) = Pr(|X-350| \geq \frac{100}{\sqrt{291+2/3}}\sqrt{291+ 2/3}) \leq \frac{291+2/3}{10000}
\]

And by the Hoeffding inequality
$$Pr(|X-350| \geq 100) \leq 2 \exp \bigg( -\frac{20000}{\sum_{n=1}^{100}(6-1)^2} \bigg) = 2 \exp (-8)$$

\color{black}

\item
Let $\mathcal X$ be the vector space of \emph{finitely} nonzero sequences $X=(x_1, x_2, \ldots, x_n, 0, 0, \ldots)$.
Define the norm on $\mathcal X$ as $\|X\|=\max |x_i|$.
Let $X_n$ be a point in $\mathcal X$ (a sequence) defined by
$$X_n = \left(1, \frac{1}{2}, \frac{1}{3}, \ldots, \frac{1}{n}, 0, 0, \ldots \right).$$

\begin{itemize}
\item
Show that the sequence $X_n$ is a Cauchy sequence.

\color{blue}
The sequence $X_n$ is a Cauchy sequence. Let $\epsilon > 0$ be given. 
Then let $N= \lceil 1/\epsilon \rceil$. Then for any $l,s > N$
such that $l \leq s$
$$
\|x_s - x_l\| = \|(0,0,...,1/(l+1), ..., 1/s,0,...,0)\| = 1/(l+1) < 1/N \leq \epsilon
$$
\color{black}

\item
Show that $\mathcal X$ is not complete.
\end{itemize}

\color{blue}

$\mathcal X$ is not complete since $\|X_n - X_{n-1}\|$ converges to 0 which implies that as $n \to \infty, X_{n-1} \to X_n$ and this would imply that the number of non-zero elements of $X_{n-1}$ would have to go to $\infty$, which is not finite, and so $X_{n-1}$ is not in $\mathcal X$. 
\color{black}



\item
Determine the range and nullspace of the following linear operators (matrices):
$$A=
\begin{bmatrix}
1 & 0 \\
5 & 4 \\
2 & 4
\end{bmatrix}
\quad
B=
\begin{bmatrix}
1 & 0 & 1 \\
5 & 4 & 9 \\
2 & 4 & 6
\end{bmatrix}
$$

\color{blue}
$A$ has a null space of $\vec{0}$ and range equal to $\text{span}([1,5,2]^\top, [0,4,4]^\top)$

$B$ has a null space equal to $\text{span}([-1,-1,1]^\top)$ and a range equal to $\text{span}([1,5,2]^\top, [0,4,4]^\top)$.
\color{black}


\item
Let 
$$A=\begin{bmatrix}
1 & 4 & 5 & 6 \\
6 & 7 & 2 & 1
\end{bmatrix}
\quad
b=\begin{bmatrix}
48\\
30
\end{bmatrix}.$$
One solution to $Ax=b$ is $x=[1,2,3,4]^\top$.  Compute the least-squares solution using the SVD (explain how), and compare. Why was the solution chosen?

\color{blue}
Using SVD $A$ is equivalent to 
$$
U \Sigma V
$$
where $U$ and $V$ are orthogonal matrices and $\Sigma$ is a diagonal matrix with diagonal values $\sigma_1, \sigma_2$ . 
Then solving $Ax = b$ is equivalent to minimzing $\lVert Ax - b \rVert ^ 2$ which is equivalent to,
$$
\lVert U \Sigma V x - b \rVert ^ 2
$$
Since $U^\top$ is orthogonal it does not change the norm so the above is equivalent to,
$$
\lVert U^T U \Sigma V x - U^\top b \rVert ^2 = \lVert \Sigma V x - U^\top b \rVert ^2
$$
Then let $z = V x$ and the above is equivalent to 
$$
(\sigma_1 z_1 - U_1^\top b)^2 + (\sigma_2 z_1 - U_2^\top b)^2  + (U_3^T b)^2 + (U_3^T b)^2
$$
Since this is a minimization we can remove the summands that don't involve $z$ and get
$$
(\sigma_1 z_1 - U_1^\top b)^2 + (\sigma_2 z_2 - U_2^\top b)^2 
$$
We can then clearly minimize this by setting the free variables $z_3 = z_4 = 0$ and
$z_1 = \frac{U_1^\top b}{\sigma_1}$ , $z_2 = \frac{U_2^\top b}{\sigma_2}$
Solving for $z$ we get
$$
z = [-4.68423189, -2.75801453,  0,  0]
$$  
and 
$$
x = V^\top z = [ 0.54424779, 2.40265487, 3.09292035, 3.7300885]
$$
Since $\lVert z \rVert = \lVert Vx \rVert = \lVert x \rVert$,
Then since $z_1, z_2$ are constrained and $z_3, z_4$ are free, then by setting $z_3=z_4=0$ we minimize $\lVert z \rVert$ and thereby minimize $\lVert x \rVert$. Therefore SVD gives the minimimum norm solution to $Ax = b$ .
\color{black}


\item
Consider the following process.  A probability vector $p=(p_1, \ldots, p_d)$ is drawn from a Dirichlet distribution with parameter vector $\alpha$.
Then, a vector of category counts $x=(x_1, \ldots, x_d)$ is drawn from a multinomial distribution with probability vector $p$ and number of trials $N$. Give an analytic form of $P(x \mid \alpha)$.  

\color{blue}
Since our $p$ vector is random then

$$P(x | \alpha) = \int_p multinomial(x | p) * dirichlet(p|\alpha) dp $$
$$
=\frac{\Gamma(\sum_i \alpha_i)d!}{\prod_i\Gamma(\alpha_i)*\prod_i x_i !} \int_p \prod_i p_i ^ {\alpha_i -1} \prod_i p_i ^{x_i}
$$
$$
=\frac{\Gamma(\sum_i \alpha_i)d!}{\prod_i\Gamma(\alpha_i)*\prod_i x_i !} \int_p \prod_i p_i ^ {\alpha_i + x_i -1} 
$$
We then noticed that the integral is that of an unormalized dirichlet pmf with $\alpha_i' = \alpha_i + x_i$
Therefore the above becomes,
$$
\frac{\prod_i(\Gamma(a_i + x_i)\Gamma(\sum_i \alpha_i)d!}{\Gamma(\sum_i a_i + x+i) \prod_i\Gamma(\alpha_i)*\prod_i x_i !}
$$

\color{black}
\item
Let $X_1, X_2, \ldots, X_m$ be a random sample, where $X_i \sim U(0,\theta)$ the uniform distribution.
\begin{itemize}
\item Show that $\hat\theta_{ML} = \max X_i$.
\item Show that the density of $\hat\theta_{ML}$ is $f_\theta(x) = \frac{m}{\theta^m} x^{m-1}$.
\item Find the expected value of $\hat\theta_{ML}$.
\item Find the variance of $\hat\theta_{ML}$.
\end{itemize}

\color{blue}
Under the uniform distribution the likelihood for $\theta > 0$ is equal to
$$
L(\theta | x_1, ...,x_m) = \prod_{i = 1}^m \mathbbm{1}(x_i <= \theta) * 1/\theta 
$$
Therefore clearly since $\theta$ is positve the maximum likelihood estimator is max$X_i$

Since $\hat{\theta}_{ML}$ is the max order statistic then it has density,
\[
f_\theta(x) = m * F(x)^{m-1} * f(x-1) = m * (x/\theta)^{m-1} * 1/\theta = \frac{m}{\theta^m}x^{m-1}
\]

\[
E[\hat{\theta}_{ML}] = \int_0^\theta = x * \frac{m}{\theta^m}x^{m-1} = \frac{m}{m+1} \theta
\]

\[
E[\hat{\theta}_{ML}^2] = \int_0^\theta = x^2 * \frac{m}{\theta^m}x^{m-1} = \frac{m}{m+2} \theta ^2
\]

\[
Var[\hat\theta_{ML}] = E[\hat{\theta}_{ML}^2] - E[\hat{\theta}_{ML}]^2 = (\frac{m}{m+2} - \frac{m^2}{(m+1)^2}) \theta^2
\]

\color{black}

\item
Let $X_1, \ldots, X_n$ be a sample from $N(\mu, \sigma^2)$.
\begin{itemize}
\item Show that the MLE of $\sigma^2$ is
$${\hat\sigma}^2 = n^{-1} \sum_{i=1}^n (X_i - \bar X)^2.$$

\color{blue}
Since the maximum likelihood of $\sigma^2$ is dependent on $\mu$ we must consider a likelihood funciton that is a function of $\hat{\sigma}^2$ and $\hat{\mu}$. So for $\hat{\mu}$ we will use the MLE of $\mu$, which is $\bar{X}$. 
Then the likelihood function is 
$$
L(X | \sigma^2 = \hat{\sigma}^2, \mu = \hat{\mu}) = (2\pi \hat \sigma^2)^{-n/2} \exp\big(-1/2\hat \sigma^2 * \sum_i (X_i - \hat{\mu})^2 \big)
$$ 

Minimizing this is equivalent to minimimizing the log,
$$
-n/2 * log(2\pi) -n/2 *log(\hat\sigma^2) - 1/2 \hat\sigma^2 * \sum_i (X_i - \hat\mu)^2
$$
The derivative of this with respect to $\hat\sigma^2$ is ,
$$\frac{-n}{2\hat\sigma^2} + \frac{\sum_i (X_i - \hat\mu)^2}{2 \hat\sigma^2}$$
which when $\hat\mu = \bar{X}$ and 
$\hat\sigma^2 = n^{-1} \sum_i (X_i - \bar{X})^2$ is equal to 


$$
\frac{-n^2}{2 \sum_i (X - \bar{X})^2} + \frac{n^2 * \sum_i (X- \bar{X})^2}{2 \big(\sum_i (X_i - \bar {X})^2 \big)^2} = 0
$$

\color{black}


\item Show that ${\hat\sigma}^2$ has a smaller mean squared error than 
$$(n-1)^{-1} \sum_{i=1}^n (X_i - \bar X)^2.$$
\end{itemize}
\color{blue}
We know that the above is the sample variance, which we will call $\hat\sigma_{sv}^2$,
has the properties,

$$E[\hat\sigma^2_{sv}] = \sigma^2$$
$$Var(\hat\sigma^2_{sv}) =  2\sigma^4/(n-1)$$

Since $$\hat\sigma ^2 = (n-1)/n * \hat\sigma^2_{sv}$$
then,
$$
E[\hat\sigma^2] = (n-1)/n * \sigma^2
$$
$$
Var(\hat\sigma^2) = (n-1)^2/n^2 * 2\sigma^4/(n-1) = \frac{2(n-1)\sigma^4}{n^2}
$$
The MSE of an estimator is its variance plus bias squared so,
$$
MSE(\hat\sigma^2_{sv}) = 2\sigma^4/(n-1)
$$
and,
$$
MSE(\hat\sigma^2) = \frac{2(n-1)\sigma^4}{n^2} + (\sigma^2/n)^2 = \frac{\sigma^4 (2n-1)}{n^2} < 2\sigma^4 / n
$$
for positive $n$, $2\sigma^4/n < 2\sigma^4 / (n-1)$
and then
$MSE(\hat\sigma^2) < MSE(\hat\sigma^2_{sv})$.  

\color{black}

\item
Consider the directed graphical model in which none of the variables is observed.  
$$
\begin{array}{cc}
a \searrow &  \\
& c \rightarrow d \\
b \nearrow &
\end{array}
$$
Show that $a \bot b \vert \emptyset$ by using a probability argument.
Suppose we now observe the variable $d$.  Show that in general $a \not\perp b \vert d$ (you can use a counterexample).

\item
Consider two discrete random variables $x,y \in \{A,B,C\}$.
Construct a joint distribution $p(x,y)$ with the following properties:
\begin{itemize}
\item $\hat x$ is the maximizer of the marginal $p(x)$
\item $\hat y$ is the maximizer of the marginal $p(y)$
\item $p(\hat x, \hat y)=0$.
\end{itemize}
\color{blue}
Let our joint probability be 
$$
P(x,y) = 
\begin{cases}
1/3 & x = A, y \neq B \\
1/6 & y = C, x \neq A \\
0 & else
\end{cases}
$$
For this distribution $A$ is a maximizer for $x$ with $p(x = A) = 2/3$ and $B$ is a maximizer of $y$ with $p(y = B) = 1/3$. 
But $p(X = A, Y = B) = 0$
\color{black}

\item
Logistic regression for $y\in \{-1,1\}$ is defined by
$$p(y \mid x; w,b) = \frac{1}{1+e^{-y (x^\top w + b)}}.$$
Show that logistic regression is in the exponential family, that is, the probability distribution can be written in the form
$$p(y \mid x; \tilde w) = \frac{1}{Z(x,\tilde w)} e^{\phi(y,x)^\top \tilde w}.$$
Note the mapping $\phi$ depends only on $y, x$, but not on $w$ or $b$.

\color{blue}
Note that the above probability is equal to 
$$
\frac{e^{1/2 y(x^\top w + b}}{e^{1/2 y(x^\top w + b)}} * \frac{1}{1+e^{-y (x^\top w + b)}}
$$
$$
= \frac{e^{1/2 y(x^\top w + b)}}{e^{1/2 y(x^\top w + b)} +e^{-1/2 y(x^\top w + b)} }
$$

Since $y \in \{1, -1\}$ this is equivalent to 
$$
\frac{e^{1/2 y(x^\top w + b)}}{e^{1/2(x^\top w + b)} +e^{-1/2(x^\top w + b)} }
$$

Then let $\tilde w = [w,b]$, $Z(x,\tilde w) = e^{1/2(x^\top w + b)} +e^{-1/2(x^\top w + b)}$ and $\phi(y,x) = [yx/2, y/2]$,

Then the above is equivalent to 
$$
\frac{1}{Z(x,\tilde w)} e^{\phi(y,x)^\top \tilde w}
$$
and therefore the probability belongs to the exponential family.

\color{black}

\end{enumerate}
\end{document}
